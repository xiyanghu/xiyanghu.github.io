---
---

@string{NeurIPS = {Advances in Neural Information Processing Systems,}}

@misc{chen2026valueactionalignmentlargelanguage,
      title={Value-Action Alignment in Large Language Models under Privacy-Prosocial Conflict}, 
      author={Guanyu Chen and Chenxiao Yu and Xiyang Hu},
      year={2026},
      eprint={2601.03546},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      html={https://arxiv.org/abs/2601.03546}, 
      abbr={preprint},
      selected={true},
      preview={vaar.png},
}

@misc{liu2025topologymattersmeasuringmemory,
      title={Topology Matters: Measuring Memory Leakage in Multi-Agent LLMs}, 
      author={Jinbo Liu and Defu Cao and Yifei Wei and Tianyao Su and Yuan Liang and Yushun Dong and Yue Zhao and Xiyang Hu},
      year={2025},
      eprint={2512.04668},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      html={https://arxiv.org/abs/2512.04668}, 
      abbr={preprint},
      selected={true},
      preview={mama.png},
}

@inproceedings{yang2025ad,
  title={AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection},
  author={Yang, Tiankai and Liu, Junjun and Siu, Wingchun and Wang, Jiahang and Qian, Zhuangzhuang and Song, Chanjuan and Cheng, Cheng and Hu, Xiyang and Zhao, Yue},
  booktitle={Findings of the 14th International Joint Conference on Natural Language Processing and the 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL)},
  year={2025},
  html={https://arxiv.org/abs/2505.12594}, 
  abbr={IJCNLP-AACL},
  selected={true},
  preview={adagent.png},
}

@misc{xing2025llmsreliablerankersrank,
      title={Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization}, 
      author={Tiancheng Xing and Jerry Li and Yixuan Du and Xiyang Hu},
      year={2025},
      eprint={2510.06732},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      html={https://arxiv.org/abs/2510.06732}, 
      abbr={preprint},
      selected={true},
      preview={raf.png}, 
}

@InProceedings{li2025mitigatinghallucinationslargelanguage,
      title={Mitigating Hallucinations in Large Language Models via Causal Reasoning}, 
      author={Yuangang Li and Yiqing Shen and Yi Nian and Jiechao Gao and Ziyi Wang and Chenxiao Yu and Shawn Li and Jie Wang and Xiyang Hu and Yue Zhao},
      year={2026},
      booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
      html={https://arxiv.org/abs/2508.12495}, 
      abbr={AAAI},
      selected={true},
      preview={cdcr.png}, 
}

@misc{tang2025stealthrankllmrankingmanipulation,
      title={StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization}, 
      author={Yiming Tang and Yi Fan and Chenxiao Yu and Tiankai Yang and Yue Zhao and Xiyang Hu},
      year={2025},
      eprint={2504.05804},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      html={https://arxiv.org/abs/2504.05804}, 
      preview={stealthrank.png}, 
      abbr={preprint},
      selected={true},
}

@misc{hu2025dynamicsadversarialattackslarge,
  title={Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines}, 
  author={Xiyang Hu},
  year={2025},
  eprint={2501.00745},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  html={https://arxiv.org/abs/2501.00745},
  preview={dynamics.png}, 
  abbr={preprint},
  selected={true},
}

@misc{yu2025largescalesimulationlargelanguage,
  title={A Large-Scale Simulation on Large Language Models for Decision-Making in Political Science}, 
  author={Chenxiao Yu and Jinyi Ye and Yuangang Li and Zhaotian Weng and Zheng Li and Emilio Ferrara and Xiyang Hu and Yue Zhao},
  year={2025},
  eprint={2412.15291},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  html={https://arxiv.org/abs/2412.15291}, 
  preview={election.png},
  abbr={preprint},
  selected={false},
}

@article{li2025drugagent,
  title={DrugAgent: A Theory-Driven LLM Multi-Agent System for Automating Machine Learning Programming in Drug Discovery},
  author={Li, Jiate and Liu, Sizhe and Hu, Xiyang and Zhao, Jieyu and Zhao, Yue},
  journal={Available at SSRN 5746063},
  year={2025},
  html={https://ssrn.com/abstract=5746063}, 
  preview={drugagent.png},
  abbr={preprint},
  selected={true},
}

@misc{liu2025drugagentautomatingaiaideddrug,
  title={DrugAgent: Automating AI-aided Drug Discovery Programming through LLM Multi-Agent Collaboration}, 
  author={Sizhe Liu and Yizhou Lu and Siyu Chen and Xiyang Hu and Jieyu Zhao and Yingzhou Lu and Yue Zhao},
  year={2025},
  eprint={2411.15692},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  html={https://arxiv.org/abs/2411.15692}, 
  abbr={preprint},
  selected={false},
}

@InProceedings{li2025secureondevicevideoood,
    author    = {Shawn Li and Peilin Cai and Yuxiao Zhou and Zhiyu Ni and Renjie Liang and You Qin and Yi Nian and Zhengzhong Tu and Xiyang Hu and Yue Zhao},
    title     = {Secure On-Device Video OOD Detection Without Backpropagation},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2025},
    pages     = {},
    abbr={ICCV},
    preview={secdood.png},
    selected={true},
}

@misc{huang2025trustworthinessgenerativefoundationmodels,
  title={On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective}, 
  author={Yue Huang and Chujie Gao and Siyuan Wu and Haoran Wang and Xiangqi Wang and Yujun Zhou and Yanbo Wang and Jiayi Ye and Jiawen Shi and Qihui Zhang and Yuan Li and Han Bao and Zhaoyi Liu and Tianrui Guan and Dongping Chen and Ruoxi Chen and Kehan Guo and Andy Zou and Bryan Hooi Kuen-Yew and Caiming Xiong and Elias Stengel-Eskin and Hongyang Zhang and Hongzhi Yin and Huan Zhang and Huaxiu Yao and Jaehong Yoon and Jieyu Zhang and Kai Shu and Kaijie Zhu and Ranjay Krishna and Swabha Swayamdipta and Taiwei Shi and Weijia Shi and Xiang Li and Yiwei Li and Yuexing Hao and Yuexing Hao and Zhihao Jia and Zhize Li and Xiuying Chen and Zhengzhong Tu and Xiyang Hu and Tianyi Zhou and Jieyu Zhao and Lichao Sun and Furong Huang and Or Cohen Sasson and Prasanna Sattigeri and Anka Reuel and Max Lamparth and Yue Zhao and Nouha Dziri and Yu Su and Huan Sun and Heng Ji and Chaowei Xiao and Mohit Bansal and Nitesh V. Chawla and Jian Pei and Jianfeng Gao and Michael Backes and Philip S. Yu and Neil Zhenqiang Gong and Pin-Yu Chen and Bo Li and Xiangliang Zhang},
  year={2025},
  eprint={2502.14296},
  archivePrefix={arXiv},
  primaryClass={cs.CY},
  html={https://arxiv.org/abs/2502.14296}, 
  abbr={preprint},
}

@article{hu2022human,
  title={Human-Algorithmic Bias: Source, Evolution, and Impact},
  author={Hu, Xiyang and Huang, Yan and Li, Beibei and Lu, Tian},
  journal={Management Science},
  year={2025},
  abbr={MS},
  html={https://pubsonline.informs.org/doi/10.1287/mnsc.2022.03862},
  abstract={Prior work on human-algorithmic bias has seen difficulty in empirically identifying the underlying mechanisms of bias, because in a typical "one-time" decision-making scenario, different mechanisms tend to generate the same patterns of observable decisions. In this study, leveraging a unique repeat decision-making setting in a high-stakes micro-lending context, we aim to uncover the underlying source, evolution dynamics, and associated impacts of bias. We first develop and estimate a structural econometric model of the decision dynamics to understand the source and evolution of potential bias in human evaluators in microloan granting. We find that both preference-based bias and belief-based bias are present in human evaluators' decisions and are in favor of female applicants. Through counterfactual simulations, we quantify the effects of the two types of bias on both fairness and profits. The results show that the elimination of either of the two biases improves the fairness in financial resource allocation, as well as the platform profits. The profit improvement mainly stems from the increase in the approval probability for male borrowers, especially those who would eventually pay back loans. Furthermore, to examine how human biases evolve when being inherited by machine learning (ML) algorithms, we then train a set of state-of-the-art ML algorithms for default risk prediction on both real-world datasets with human biases encoded within and counterfactual datasets with human biases partially or fully removed. By comparing the decision outcomes in different counterfactual settings, we find that even fairness-unaware ML algorithms can reduce bias present in human loan-granting decisions. Interestingly, while removing both types of human biases from the training data can further improve ML fairness, the fairness-enhancing effects vary significantly between new and repeat applicants. Based on our findings, we discuss how to reduce decision bias most effectively in a human-machine learning pipeline.},
  preview={humanmachine.png},
  selected={true},
}

@misc{li2024politicalllmlargelanguagemodels,
  title={Political-LLM: Large Language Models in Political Science}, 
  author={Lincan Li and Jiaqi Li and Catherine Chen and Fred Gui and Hongjia Yang and Chenxiao Yu and Zhengguang Wang and Jianing Cai and Junlong Aaron Zhou and Bolin Shen and Alex Qian and Weixin Chen and Zhongkai Xue and Lichao Sun and Lifang He and Hanjie Chen and Kaize Ding and Zijian Du and Fangzhou Mu and Jiaxin Pei and Jieyu Zhao and Swabha Swayamdipta and Willie Neiswanger and Hua Wei and Xiyang Hu and Shixiang Zhu and Tianlong Chen and Yingzhou Lu and Yang Shi and Lianhui Qin and Tianfan Fu and Zhengzhong Tu and Yuzhe Yang and Jaemin Yoo and Jiaheng Zhang and Ryan Rossi and Liang Zhan and Liang Zhao and Emilio Ferrara and Yan Liu and Furong Huang and Xiangliang Zhang and Lawrence Rothenberg and Shuiwang Ji and Philip S. Yu and Yue Zhao and Yushun Dong},
  year={2024},
  eprint={2412.06864},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  html={https://arxiv.org/abs/2412.06864}, 
  abbr={preprint},
}

@misc{yu2024accuratepresidentialelectionmultistep,
  title={Towards More Accurate US Presidential Election via Multi-step Reasoning with Large Language Models}, 
  author={Chenxiao Yu and Zhaotian Weng and Yuangang Li and Zheng Li and Xiyang Hu and Yue Zhao},
  year={2024},
  eprint={2411.03321},
  archivePrefix={arXiv},
  primaryClass={cs.AI},
  html={https://arxiv.org/abs/2411.03321}, 
  abbr={preprint},
}

@inproceedings{yang-etal-2025-ad,
    title = "{AD}-{LLM}: Benchmarking Large Language Models for Anomaly Detection",
    author = "Yang, Tiankai  and
      Nian, Yi  and
      Li, Li  and
      Xu, Ruiyao  and
      Li, Yuangang  and
      Li, Jiaqi  and
      Xiao, Zhuo  and
      Hu, Xiyang  and
      Rossi, Ryan A.  and
      Ding, Kaize  and
      Hu, Xia  and
      Zhao, Yue",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.79/",
    doi = "10.18653/v1/2025.findings-acl.79",
    pages = "1524--1547",
    ISBN = "979-8-89176-256-5",
    abstract = "Anomaly detection (AD) is an important machine learning task with many real-world uses, including fraud detection, medical diagnosis, and industrial monitoring. Within natural language processing (NLP), AD helps detect issues like spam, misinformation, and unusual user activity. Although large language models (LLMs) have had a strong impact on tasks such as text generation and summarization, their potential in AD has not been studied enough. This paper introduces AD-LLM, the first benchmark that evaluates how LLMs can help with NLP anomaly detection. We examine three key tasks: (i) zero-shot detection, using LLMs' pre-trained knowledge to perform AD without tasks-specific training; (ii) data augmentation, generating synthetic data and category descriptions to improve AD models; and (iii) model selection, using LLMs to suggest unsupervised AD models. Through experiments with different datasets, we find that LLMs can work well in zero-shot AD, that carefully designed augmentation methods are useful, and that explaining model selection for specific datasets remains challenging. Based on these results, we outline six future research directions on LLMs for AD."
}

@inproceedings{li2024nlpadbenchnlpanomalydetection,
    title = "NLP-ADBench: NLP Anomaly Detection Benchmark",
    author = "Yuangang Li and Jiaqi Li and Zhuo Xiao and Tiankai Yang and Yi Nian and Xiyang Hu and Yue Zhao",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2025",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics"
}

@misc{liu2024coodconceptbasedzeroshotood,
  title={COOD: Concept-based Zero-shot OOD Detection}, 
  author={Zhendong Liu and Yi Nian and Henry Peng Zou and Li Li and Xiyang Hu and Yue Zhao},
  year={2024},
  eprint={2411.13578},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  html={https://arxiv.org/abs/2411.13578}, 
}

@article{JMLR:v25:23-0963,
  author  = {Kay Liu and Yingtong Dou and Xueying Ding and Xiyang Hu and Ruitong Zhang and Hao Peng and Lichao Sun and Philip S. Yu},
  title   = {PyGOD: A Python Library for Graph Outlier Detection},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {141},
  pages   = {1--9},
  html     = {http://jmlr.org/papers/v25/23-0963.html},
  abbr={JMLR},
  preview={pygod.png},
  selected={true},
}

@article{jiang2023adgym,
  title={ADGym: Design Choices for Deep Anomaly Detection},
  author={Jiang, Minqi and Hou, Chaochuan and Zheng, Ao and Han, Songqiao and Huang, Hailiang and Wen, Qingsong and Hu, Xiyang and Zhao, Yue},
  journal={Advances in Neural Information Processing Systems},
  year={2023},
  selected={false},
  abbr={NeurIPS},
  abstract={Deep learning (DL) techniques have recently been applied to anomaly detection (AD), with numerous successful applications in finance, medical service, cloud computing, etc. However, current research often directly evaluates a deep AD algorithm as a whole, which cannot disentangle the contribution of each design choice (e.g., loss functions and network architectures). Meanwhile, we may neglect the contribution of other meaningful prerequisite steps like preprocessing by giving all credits to newly designed loss functions and/or architectures. In this paper, we address the above gaps by answering: (i) which components (i.e., design choices) of deep AD methods play crucial roles in detecting anomalies? (ii) how can we build more effective AD algorithms given different datasets by automatically choosing the optimal design choices other than using existing off-the-shelf ones? To this end, we propose ADGym, the first and fully open-source design platform for large evaluation and automatic selection of AD design choices. Extensive experiments show that directly using existing leading methods is not optimal, and the AD models composed by ADGym significantly outperform state-of-the-art methods.}
}

@article{hu2022language,
  title={Language Agnostic Multilingual Information Retrieval with Contrastive Learning},
  author={Hu, Xiyang and Chen, Xinchi and Qi, Peng and Kong, Deguang and Liu, Kunlun and Wang, William Yang and Huang, Zhiheng},
  journal={Annual Meeting of the Association for Computational Linguistics - Findings of ACL},
  year={2023},
  selected={false},
  abbr={ACL},
  preview={multilingual.png},
  html={https://arxiv.org/abs/2210.06633},
  abstract={Multilingual information retrieval (IR) is challenging since annotated training data is costly to obtain in many languages. We present an effective method to train multilingual IR systems when only English IR training data is available, by leveraging parallel and non-parallel corpora to improve the pretrained multilingual language models’ cross-lingual transfer ability. We design a semantic contrastive loss to align representations of parallel sentences that share the same semantics in different languages, and a new language contrastive loss to leverage parallel sentence pairs to remove language-specific information in sentence representations from non-parallel corpora. When trained on English IR data with these losses and evaluated zero-shot on non-English data, our model demonstrates significant improvement to prior work on retrieval performance, while it requires much less computational effort. Our model can work well even with a small number of parallel sentences, and it can be used as an add-on module to any backbone and other tasks.}
}

@article{hu2023inclusive,
  title={Inclusive Decision Making via Contrastive Learning and Domain Adaptation},
  author={Hu, Xiyang and Huang, Yan and Li, Beibei and Lu, Tian},
  journal={MIS Quarterly (Under Major Revision)},
  year={2023},
  abbr={preprint},
  selected={false},
  html={https://ssrn.com/abstract=4496106}
}

@article{jiang2023weakly,
  title={Weakly Supervised Anomaly Detection: A Survey},
  author={Jiang, Minqi and Hou, Chaochuan and Zheng, Ao and Hu, Xiyang and Han, Songqiao and Huang, Hailiang and He, Xiangnan and Yu, Philip S and Zhao, Yue},
  journal={arXiv preprint arXiv:2302.04549},
  year={2023},
  selected={false},
  abbr={preprint},
  html={https://arxiv.org/abs/2302.04549}
}

@article{han2022adbench,
  title={ADBench: Anomaly Detection Benchmark},
  author={Hu, Xiyang and Han, Songqiao and Huang, Hailiang and Jiang, Mingqi and Zhao, Yue},
  journal={Advances in Neural Information Processing Systems},
  year={2022},
  selected={false},
  abbr={NeurIPS},
  html={https://arxiv.org/abs/2206.09426},
  code = {https://github.com/Minqi824/ADBench},
  abstract={Given a long list of anomaly detection algorithms developed in the last few decades, how do they perform with regard to (i) varying levels of supervision, (ii) different types of anomalies, and (iii) noisy and corrupted data? In this work, we answer these key questions by conducting (to our best knowledge) the most comprehensive anomaly detection benchmark with 30 algorithms on 55 benchmark datasets, named ADBench. Our extensive experiments (93,654 in total) identify meaningful insights into the role of supervision and anomaly types, and unlock future directions for researchers in algorithm selection and design. With ADBench, researchers can easily conduct comprehensive and fair evaluations for newly proposed methods on the datasets (including our contributed ones from natural language and computer vision domains) against the existing baselines. To foster accessibility and reproducibility, we fully open-source ADBench and the corresponding results.}
}

@article{liu2022benchmarking,
  title={Benchmarking Node Outlier Detection on Graphs},
  author={Liu, Kay and Dou, Yingtong and Zhao, Yue and Ding, Xueying and Hu, Xiyang and Zhang, Ruitong and Ding, Kaize and Chen, Canyu and Peng, Hao and Shu, Kai and others},
  journal={Advances in Neural Information Processing Systems},
  year={2022},
  selected={false},
  abbr={NeurIPS},
  html={https://arxiv.org/abs/2206.10071},
  code = {https://github.com/pygod-team/pygod/tree/main/benchmark},
  abstract={Graph outlier detection is an emerging but crucial machine learning task with numerous applications.
Despite the proliferation of algorithms developed in recent years,
the lack of a standard and unified setting for performance evaluation limits their advancement and  usage in real-world applications.
To tap the gap, we present, (to our best knowledge) the first comprehensive unsupervised node outlier detection benchmark for graphs called UNOD, with the following highlights:
(1) evaluating fourteen methods with backbone spanning from classical matrix factorization to the latest graph neural networks;
(2) benchmarking the method performance with different types of injected outliers and organic outliers on real-world datasets;
(3) comparing the efficiency and scalability of the algorithms by runtime and GPU memory usage on synthetic graphs at different scales.
Based on the analyses of extensive experimental results, we discuss the pros and cons of current UNOD methods and point out multiple crucial and promising future research directions.}
}

@article{hu2022credit,
  title={Credit Risk Modeling without Sensitive Features: An Adversarial Deep Learning Model for Fairness and Profit},
  author={Hu, Xiyang and Huang, Yan and Li, Beibei and Lu, Tian},
  journal={International Conference on Information Systems},
  abstract={We propose an adversarial deep learning model for credit risk modeling. We make use of sophisticated machine learning model’s ability to triangulate (i.e., infer the sensitive group affiliation by using only permissible features), which is often deemed “troublesome” in fair machine learning research, in a positive way to increase both borrower welfare and lender profits while improving fairness. We train and test our model on a dataset from a real-world microloan company. Our model significantly outperforms regular deep neural networks without adversaries and the most popular credit risk model XGBoost, in terms of both improving borrowers’ welfare and lenders’ profits. Our empirical findings also suggest that the traditional AUC metric cannot reflect a model's performance on the borrowers’ welfare and lenders’ profits. Our framework is ready to be customized for other microloan firms, and can be easily adapted to many other decision-making scenarios.},
  year={2022},
  selected={false},
  abbr={ICIS},
  html={https://aisel.aisnet.org/icis2022/ai_business/ai_business/4/}
}

@article{li2022ecod,
  title={ECOD: Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions},
  author={Li, Zheng and Zhao, Yue and Hu, Xiyang and Botta, Nicola and Ionescu, Cezar and Chen, George H},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2022},
  selected={false},
  abbr={TKDE},
  html={https://ieeexplore.ieee.org/document/9737003},
  code={https://github.com/yzhao062/pyod/blob/master/pyod/models/ecod.py},
  abstract={Outlier detection refers to the identification of data points that deviate from a general data distribution. Existing unsupervised approaches often suffer from high computational cost, complex hyperparameter tuning, and limited interpretability, especially when working with large, high-dimensional datasets. To address these issues, we present a simple yet effective algorithm called ECOD (Empirical-Cumulative-distribution-based Outlier Detection), which is inspired by the fact that outliers are often the "rare events" that appear in the tails of a distribution. In a nutshell, ECOD first estimates the underlying distribution of the input data in a nonparametric fashion by computing the empirical cumulative distribution per dimension of the data. ECOD then uses these empirical distributions to estimate tail probabilities per dimension for each data point. Finally, ECOD computes an outlier score of each data point by aggregating estimated tail probabilities across dimensions. Our contributions are as follows: (1) we propose a novel outlier detection method called ECOD, which is both parameter-free and easy to interpret; (2) we perform extensive experiments on 30 benchmark datasets, where we find that ECOD outperforms 11 state-of-the-art baselines in terms of accuracy, efficiency, and scalability; and (3) we release an easy-to-use and scalable (with distributed support) Python implementation for accessibility and reproducibility.}
}

@article{hu2021icis,
  abbr={ICIS},
  title={Uncovering the Source of Evaluation Bias in Micro-Lending},
  author={Hu, Xiyang and Huang, Yan and Li, Beibei and Lu, Tian},
  journal={International Conference on Information Systems},
  abstract={We develop a structural econometric model to capture the decision dynamics of human evaluators on an online micro-lending platform, and estimate the model parameters using real-world data. We find two types of biases in gender, i.e. preference-based bias and belief-based bias, are present in human evaluators' decisions. Both types of biases are in favor of female applicants. Through counterfactual simulations, we quantify the effect of gender bias on loan granting outcomes and the welfare of the company and the borrowers. Our results imply that both the existence of the preference-based bias and that of the belief-based bias reduce the company's profits. When the preference-based bias is removed, the company earns more profits. When the belief-based bias is removed, the company's profits also increase. Both increases result from lowering the approval probability for borrowers, especially female borrowers, who eventually default on loans. For borrowers, the elimination of either bias decreases the gender gap in the credit risk evaluation.},
  year={2021},
  selected={false},
  html={https://aisel.aisnet.org/icis2021/fintech/fintech/15/}
}

@article{zhao2020suod,
  abbr={MLSys},
  title={SUOD: Accelerating Large-scale Unsupervised Heterogeneous Outlier Detection},
  author={Hu, Xiyang and Zhao, Yue and Cheng, Cheng and Wang, Cong and Wan, Changlin and Wang, Wen and Yang, Jianing and Bai, Haoping and LI, Zheng and Xiao, Cao and Wang, Yunlong and Qiao, Zhi and Sun, Jimeng and Akoglu, Leman},
  journal={Conference on Machine Learning and Systems},
  abstract={Outlier detection (OD) is a key data mining task for identifying abnormal objects from general samples with numerous high-stake applications including fraud detection and intrusion detection. Due to the lack of ground truth labels, practitioners often have to build a large number of unsupervised models that are heterogeneous (i.e., different algorithms and hyperparameters) for further combination and analysis with ensemble learning, rather than relying on a single model. However, this yields severe scalability issues on high-dimensional, large datasets. How to accelerate the training and predicting with a large number of heterogeneous unsupervised OD models? How to ensure the acceleration does not deteriorate detection models' accuracy? How to accommodate the acceleration need for both a single worker setting and a distributed system with multiple workers? In this study, we propose a three-module acceleration system called SUOD (scalable unsupervised outlier detection) to address these questions. It focuses on three complementary aspects to accelerate (dimensionality reduction for high-dimensional data, model approximation for complex models, and execution efficiency improvement for taskload imbalance within distributed systems), while controlling detection performance degradation. Extensive experiments on more than 20 benchmark datasets demonstrate SUOD's effectiveness in heterogeneous OD acceleration. By the submission time, the released open-source system has been widely used with more than 700,000 times downloads. A real-world deployment case on fraudulent claim analysis at IQVIA, a leading healthcare firm, is also provided.},
  html={https://arxiv.org/abs/2003.05731},
  year={2021},
  selected={false}
}

@article{hu2021kdd,
  abbr={KDD},
  title={Uncovering the Source of Machine Bias},
  author={Hu, Xiyang and Huang, Yan and Li, Beibei and Lu, Tian},
  journal={27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Machine Learning for Consumers and Markets Workshop},
  abstract={We develop a structural econometric model to capture the decision dynamics of human evaluators on an online micro-lending platform, and estimate the model parameters using a real-world dataset. We find two types of biases in gender, preference-based bias and belief-based bias, are present in human evaluators' decisions. Both types of biases are in favor of female applicants. Through counterfactual simulations, we quantify the effect of gender bias on loan granting outcomes and the welfare of the company and the borrowers. Our results imply that both the existence of the preference-based bias and that of the belief-based bias reduce the company's profits. When the preference-based bias is removed, the company earns more profits. When the belief-based bias is removed, the company's profits also increase. Both increases result from raising the approval probability for borrowers, especially male borrowers, who eventually pay back loans. For borrowers, the elimination of either bias decreases the gender gap of the true positive rates in the credit risk evaluation. We also train machine learning algorithms on both the real-world data and the data from the counterfactual simulations. We compare the decisions made by those algorithms to see how evaluators' biases are inherited by the algorithms and reflected in machine-based decisions. We find that machine learning algorithms can mitigate both the preference-based bias and the belief-based bias.},
  year={2021},
  selected={false},
  html={https://arxiv.org/abs/2201.03092}
}


@article{hu2021cehui,
  abbr={JAH},
  title={开化寺大雄宝殿彩画测绘复原},
  author={Hu, Xiyang and others},
  journal={建筑史学刊(Journal of Architecture History)},
  year={2021},
  volume={1},
  pages = {162-176},
  selected={false},
  pdf={cehui.pdf},
  html={http://archhistory-journal.com/jzsxk/article/abstract/20210116}
}

@article{ICDM2020_a,
    abbr={ICDM},
    title = {COPOD: Copula-Based Outlier Detection},
    author = {LI, Zheng and Zhao, Yue and Botta, Nicola and Ionescu, Cezar and Hu, Xiyang},
    journal = {IEEE International Conference on Data Mining},
    year = {2020},
    abstract={Outlier detection refers to the identification of rare items that are deviant from the general data distribution. Existing approaches suffer from high computational complexity, low predictive capability, and limited interpretability. As a remedy, we present a novel outlier detection algorithm called COPOD, which is inspired by copulas for modeling multivariate data distribution. COPOD first constructs an empirical copula, and then uses it to predict tail probabilities of each given data point to determine its level of "extremeness". Intuitively, we think of this as calculating an anomalous p-value. This makes COPOD both parameter-free, highly interpretable, and computationally efficient. In this work, we make three key contributions, 1) propose a novel, parameter-free outlier detection algorithm with both great performance and interpretability, 2) perform extensive experiments on 30 benchmark datasets to show that COPOD outperforms in most cases and is also one of the fastest algorithms, and 3) release an easy-to-use Python implementation for reproducibility.},
    html={https://ieeexplore.ieee.org/document/9338429},
    selected={false}
}

@article{NIPS2019_8947,
    abbr={NeurIPS},
    title = {Optimal Sparse Decision Trees},
    author = {Hu, Xiyang and Rudin, Cynthia and Seltzer, Margo},
    journal = {Advances in Neural Information Processing Systems},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {7265--7273},
    year = {2019},
    publisher = {Curran Associates, Inc.},
    html = {http://papers.nips.cc/paper/8947-optimal-sparse-decision-trees.pdf},
    abstract={Decision tree algorithms have been among the most popular algorithms for interpretable (transparent) machine learning since the early 1980's. The problem that has plagued decision tree algorithms since their inception is their lack of optimality, or lack of guarantees of closeness to optimality: decision tree algorithms are often greedy or myopic, and sometimes produce unquestionably suboptimal models. Hardness of decision tree optimization is both a theoretical and practical obstacle, and even careful mathematical programming approaches have not been able to solve these problems efficiently. This work introduces the first practical algorithm for optimal decision trees for binary variables. The algorithm is a co-design of analytical bounds that reduce the search space and modern systems techniques, including data structures and a custom bit-vector library. Our experiments highlight advantages in scalability, speed, and proof of optimality.},
    code = {https://github.com/xiyanghu/OSDT},
    html={https://arxiv.org/abs/1904.12847},
    abbr={NeurIPS},
    preview={osdt.png},
    selected={true}
}